---
# paramaters you are most likely to want to change are in this top section

# number of training users
num_users_create: 1
# number of trainer users
num_trainers_create: 1

# control plane flavour, usually set to
# - "balanced1.2cpu4ram" for testing
# - "balanced1.4cpu8ram" for production
control_plane_flavor: balanced1.2cpu4ram

# You must have enough worker resources to accommodate the number of users.
# Currently all apps are limited to 2cpu4ram (although this may change).
# You cannot allocate the full ram on a worker node, for example we could fit
# 15x 2cpu4ram sessions on a balanced1.32cpu64ram node. It is recommended to
# use a small number of large worker nodes to avoid repeated wasting of the
# overhead, e.g.
# - 2x balanced1.32cpu64ram for up to 30 2cpu4ram sessions
# - 3x balanced1.32cpu64ram for up to 45 2cpu4ram sessions
cluster_worker_count: 1
worker_flavour: balanced1.8cpu16ram
# worker disksize is the size of the drive on the worker node, which is used
# for pulling the docker images
worker_disksize: 60

# apps config
# - `k8s_container` should be the container that will be used by the app, usually
#   defined in setup.yml in the git repo
# - `repo` is the git repo for the app, use the HTTPS address not the SSH one
# - `version` is the git tag (or commit hash) for the git `repo`
# - `enabled` apps will show up in open ondemand
# - `pre_pull` will pull (and cache) the `k8s_container` image on all worker
#   nodes, so starting those apps will be much faster (`enable_pod_pre_pull`
#   must also be true)
#
# NOTE: the k8s nodes must have enough disk space to pull all the images that
#       you set `pre_pull: true` on, do just set if for the ones you are
#       actually going to use and make sure `worker_disksize` above is big
#       enough
#
# NOTE2: it is recommended to leave the containers app disabled unless you are
#        using it, since it only works properly in privileged mode (which is
#        recommended to be turned off unless using the containers app)
ood_apps:
  jupyter_ml101:
    k8s_container: ghcr.io/nesi/training-environment-jupyter-ml101-app:v0.2.7
    repo: https://github.com/nesi/training-environment-jupyter-ml101-app.git
    version: 'v0.2.7'
    enabled: true
    pre_pull: false

  jupyter_ml102:
    k8s_container: ghcr.io/nesi/training-environment-jupyter-ml102-app:v0.3.0
    repo: https://github.com/nesi/training-environment-jupyter-ml102-app.git
    version: 'v0.3.0'
    enabled: true
    pre_pull: true

  rstudio_rnaseq:
    k8s_container: ghcr.io/nesi/training-environment-rstudio-rnaseq-app:v0.3.0
    repo: https://github.com/nesi/training-environment-rstudio-rnaseq-app.git
    version: 'v0.3.0'
    enabled: true
    pre_pull: false

  rstudio_scrnaseq:
    k8s_container: ghcr.io/nesi/training-environment-rstudio-scrnaseq-app:v0.6.0
    repo: https://github.com/nesi/training-environment-rstudio-scrnaseq-app.git
    version: 'v0.6.0'
    enabled: true
    pre_pull: false

  rstudio:
    k8s_container: ghcr.io/nesi/training-environment-rstudio-app:v0.3.0
    repo: https://github.com/nesi/training-environment-rstudio-app.git
    version: 'v0.3.0'
    enabled: true
    pre_pull: false

  shell4b:
    k8s_container: ghcr.io/nesi/training-environment-jupyter-intermediate-shell-app:v0.3.5
    repo: https://github.com/nesi/training-environment-jupyter-intermediate-shell-app.git
    version: 'v0.3.5'
    enabled: true
    pre_pull: false

  containers:
    k8s_container: ghcr.io/nesi/training-environment-jupyter-containers-app:v0.1.2
    repo: https://github.com/nesi/training-environment-jupyter-containers-app.git
    version: 'v0.1.2'
    enabled: true
    pre_pull: false

  nextflow:
    k8s_container: ghcr.io/nesi/training-environment-jupyter-intronextflow-app:v0.3.0
    repo: https://github.com/nesi/training-environment-jupyter-intronextflow-app.git
    version: 'v0.3.0'
    enabled: true
    pre_pull: false

  codeserver:
    k8s_container: ghcr.io/nesi/training-environment-codeserver-app:v0.2.0
    repo: https://github.com/nesi/training-environment-codeserver-app.git
    version: 'v0.2.0'
    enabled: true
    pre_pull: false

  intro_python:
    k8s_container: ghcr.io/nesi/training-environment-jupyter-python-app:v0.4.0
    repo: https://github.com/nesi/training-environment-jupyter-python-app.git
    version: 'v0.4.0'
    enabled: true
    pre_pull: false

  introshell:
    k8s_container: ghcr.io/nesi/training-environment-jupyter-introduction-shell-app:v0.6.0
    repo: https://github.com/nesi/training-environment-jupyter-introduction-shell-app.git
    version: 'v0.6.0'
    enabled: true
    pre_pull: false

  indigidatar:
    k8s_container: ghcr.io/nesi/training-environment-rstudio-indigidata-app:v0.5.0
    repo: https://github.com/nesi/training-environment-rstudio-indigidata-app.git
    version: 'v0.5.0'
    enabled: true
    pre_pull: false

  mpi:
    k8s_container: ghcr.io/nesi/training-environment-mpi-shell-app:v0.2.1
    repo: https://github.com/nesi/training-environment-mpi-shell-app.git
    version: 'v0.2.1'
    enabled: true
    pre_pull: false

# this is currently required for containers and nextflow apps to run properly (fakeroot)
# Note: you should probably set to false unless you are running a containers workshop
enable_privileged_pods: true

# pull the images defined in ood_apps onto all k8s worker nodes
# Note: make sure the worker nodes have enough `worker_disksize` (especially if many apps are enabled)
enable_pod_pre_pull: false

# variables to provision data into home dirs (for apps where the data is large)
provision_data_scrnaseq: false

# tuning the web node apache config, setting to 8 gives the suggested setting here:
# https://osc.github.io/ood-documentation/latest/how-tos/debug/debug-apache.html#performance-tuning
# should be good for 40-50 users
# in production you should choose a web node flavor that has at least as many CPUs
# as the value you set here
apache_server_limit: 8

# the number of nfs threads to start, default to 64
# if your app does heavy IO it *may* help to increase this and possibly also increase size
# of services node flavour
nfs_threads: 64

###############################################################################
# variables below this line should not usually need to be changed
###############################################################################

slack_notify: yes
slack_token: "CHANGEME_SLACK_TOKEN"
keycloak_admin_password: "CHANGEME_KEYCLOAK_ADMIN_PASSWORD"
ldap_admin_password: "CHANGEME_LDAP_ADMIN_PASSWORD"

# DNS Settings
route_53_zone: data.nesi.org.nz

oidc_hostname: "{{ hostvars['servicesnode']['hostname'] }}.{{ route_53_zone }}"
clouds_yaml_local_location: ~/.config/openstack/clouds.yaml

cluster_rdc_project: NeSI-Training-Test

# authentication settings
oidc_uri: "/oidc"
oidc_settings_samefile: true
oidc_provider_metadata_url: https://{{ oidc_hostname }}/realms/ondemand/.well-known/openid-configuration
oidc_client_id: ondemand.flexi
oidc_remote_user_claim: preferred_username
oidc_scope: "openid profile email"
oidc_session_inactivity_timeout: 28800
oidc_session_max_duration: 28800
oidc_cookie_same_site: "On"
oidc_settings:
  OIDCCryptoPassphrase: "CHANGEME_OIDC_CRYPTO_PASSPHRASE"
  OIDCSSLValidateServer: 'Off'
  OIDCOAuthSSLValidateServer: 'Off'
  OIDCPassClaimsAs: environment
  OIDCPassRefreshToken: 'On'
  OIDCStripCookies: mod_auth_openidc_session mod_auth_openidc_session_chunks mod_auth_openidc_session_0 mod_auth_openidc_session_1

# ood package version
#ondemand_package: 'ondemand'

# ood portal config
servername: "{{ hostvars['webnode']['hostname'] }}.{{ route_53_zone }}"
ssl_cert: /etc/letsencrypt/certs/{{ servername }}.crt
ssl_cert_key: /etc/letsencrypt/keys/{{ servername }}.key
ssl:
- "SSLCertificateFile {{ ssl_cert }}"
- "SSLCertificateKeyFile {{ ssl_cert_key }}"
httpd_auth:
  - 'AuthType openid-connect'
  - 'Require valid-user'
node_uri: '/node'
rnode_uri: '/rnode'
logout_redirect: '/oidc?logout=https%3A%2F%2F{{ servername }}'
pun_pre_hook_exports: 'OIDC_ACCESS_TOKEN,OIDC_CLAIM_EMAIL,OIDC_REFRESH_TOKEN'

cancel_session_enabled: true

httpd_port: 443
server_aliases:
  - webnode.flexi.nesi

# clusters config
clusters:
  local_linux_cluster: |
    ---
    v2:
      metadata:
        title: "Webnode"
        hidden: false
      login:
        host: 'webnode.flexi.nesi'
      job:
        adapter: "linux_host"
        submit_host: "webnode.flexi.nesi"
        ssh_hosts:
          - webnode.flexi.nesi
          - webnode
        site_timeout: 72000
        debug: true
        singularity_bin: /usr/bin/singularity
        singularity_bindpath: /etc,/media,/mnt,/opt,/run,/srv,/usr,/var
        singularity_image: /opt/ood/linuxhost_adapter/ubuntu_22.04.sif
        strict_host_checking: false
        tmux_bin: /usr/bin/tmux

# HTTPS LetsEncrypt Settings

acme_directory: https://acme-v02.api.letsencrypt.org/directory
#acme_directory: https://acme-staging-v02.api.letsencrypt.org/directory

# Clusterctl config

k8s_ood_enable: true
enable_gpu_nodes: false
use_job_pod_reaper: false

kubernetes_version: v1.30.5
capi_image_name: rocky-9-containerd-v1.30.5

capi_provider_version: v0.8.0

cluster_name: "{{ terraform_workspace }}"
cluster_namespace: default

openstack_ssh_key: CHANGEME_OPENSTACK_SSH_KEY_NAME

cluster_control_plane_count: 1

cluster_node_cidr: 10.1.0.0/24
cluster_pod_cidr: 172.0.0.0/16

bin_dir: /usr/local/bin

clusterctl_version: v1.5.1
clusterctl_repo: 'https://github.com/kubernetes-sigs/cluster-api/releases/download/{{ clusterctl_version }}/clusterctl-linux-amd64'

## Variables for OpenID Connect Configuration https://kubernetes.io/docs/admin/authentication/
## To use OpenID you have to deploy additional an OpenID Provider (e.g Dex, Keycloak, ...)

kube_oidc_auth: true
kube_oidc_url: https://{{ oidc_hostname }}/realms/ondemand
kube_oidc_client_id: kubernetes
## Optional settings for OIDC
kube_oidc_username_claim: "{{ oidc_remote_user_claim }}"
kube_oidc_username_prefix: "-"
kube_oidc_groups_claim: groups
kube_oidc_groups_prefix: 'oidc:'

# monitoring
node_exporter_version: "latest"
node_exporter_enabled_collectors:
  - systemd
  - processes
  - mountstats

prometheus_version: "2.55.1"
prometheus_scrape_configs:
  - job_name: "prometheus"
    metrics_path: "/metrics"
    static_configs:
      - targets:
        - "localhost:9090"
  - job_name: "ondemand"
    metrics_path: "/metrics"
    static_configs:
      - targets:
        - "webnode:9301"
  - job_name: "node"
    static_configs:
      - targets:
        - "servicesnode:9100"
        - "webnode:9100"

grafana_url: "https://{{ servername }}/node/servicesnode/3000/"
grafana_domain: "{{ servername }}"
grafana_server:
  serve_from_sub_path: true
grafana_version: "latest"
grafana_security:
  admin_user: "admin"
  admin_password: "CHANGEME_GRAFANA_ADMIN_PASSWORD"
grafana_alerting: {}
grafana_auth:
  anonymous:
    enabled: true
    org_name: Main Org.
    org_role: Viewer
    hide_version: true
grafana_datasources:
  - name: prometheus
    type: prometheus
    access: proxy
    url: "http://localhost:9090"
grafana_dashboards:
  - dashboard_id: '358'
    revision_id: '1'
    datasource: 'prometheus'
  - dashboard_id: '1860'
    revision_id: '37'
    datasource: 'prometheus'
  - dashboard_id: '13465'
    revision_id: '1'
    datasource: 'prometheus'
  - dashboard_id: '6257'
    revision_id: '4'
    datasource: 'prometheus'
  - dashboard_id: '19105'
    revision_id: '3'
    datasource: 'prometheus'
  - dashboard_id: '15757'
    revision_id: '37'
    datasource: 'prometheus'
  - dashboard_id: '15758'
    revision_id: '34'
    datasource: 'prometheus'
  - dashboard_id: '15759'
    revision_id: '29'
    datasource: 'prometheus'
  - dashboard_id: '15760'
    revision_id: '28'
    datasource: 'prometheus'

# Stuff to not change

ca_file_path: /usr/local/share/ca-certificates

rdc_private_key_path: "{{ ca_file_path }}/rdc-CA.key"
rdc_certificate_path: "{{ ca_file_path }}/rdc-CA.crt"
