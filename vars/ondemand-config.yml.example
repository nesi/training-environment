---
# paramaters you are most likely to want to change are in this top section

# number of training users
num_users_create: 25
# number of trainer users
num_trainers_create: 5

# control plane flavour, usually set to
# - "balanced1.2cpu4ram" for testing
# - "balanced1.4cpu8ram" for production
control_plane_flavor: balanced1.4cpu8ram

# You must have enough worker resources to accommodate the number of users.
# Currently all apps are limited to 2cpu4ram (although this may change).
# You cannot allocate the full ram on a worker node, for example we could fit
# 15x 2cpu4ram sessions on a balanced1.32cpu64ram node. It is recommended to
# use a small number of large worker nodes to avoid repeated wasting of the
# overhead, e.g.
# - 2x balanced1.32cpu64ram for up to 30 2cpu4ram sessions
# - 3x balanced1.32cpu64ram for up to 45 2cpu4ram sessions
cluster_worker_count: 4
worker_flavour: balanced1.32cpu64ram
# worker disksize is the size of the drive on the worker node, whichh is used
# for pulling the docker images
worker_disksize: 60

# apps config
# - `k8s_container` should be the container that will be used by the app, usually
#   defined in setup.yml in the git repo
# - `repo` is the git repo for the app, use the HTTPS address not the SSH one
# - `version` is the git tag (or commit hash) for the git `repo`
# - `enabled` apps will show up in open ondemand
# - `pre_pull` will pull (and cache) the `k8s_container` image on all worker
#   nodes, so starting those apps will be much faster (`enable_pod_pre_pull`
#   must also be true)
#
# NOTE: at this time the k8s nodes only have 30GB disk space, do not set all
#       images to pre_pull otherwise the pre-puller will fail, just pick the
#       one you are actually going to use
#
# NOTE2: it is recommended to leave the containers app disabled unless you are
#        using it, since it only works properly in privileged mode (which is
#        recommended to be turned off unless using the containers app)
ood_apps:
  jupyter_ml101:
    k8s_container: ghcr.io/nesi/training-environment-jupyter-ml101-app:v0.2.2
    repo: https://github.com/nesi/training-environment-jupyter-ml101-app.git
    version: 'v0.2.2'
    enabled: true
    pre_pull: false

  jupyter_ml102:
    k8s_container: ghcr.io/nesi/training-environment-jupyter-ml102-app:v0.2.0
    repo: https://github.com/nesi/training-environment-jupyter-ml102-app.git
    version: 'v0.2.0'
    enabled: true
    pre_pull: true

  rstudio_rnaseq:
    k8s_container: ghcr.io/nesi/training-environment-rstudio-rnaseq-app:v0.2.2
    repo: https://github.com/nesi/training-environment-rstudio-rnaseq-app.git
    version: 'v0.2.2'
    enabled: true
    pre_pull: false

  rstudio_scrnaseq:
    k8s_container: ghcr.io/nesi/training-environment-rstudio-scrnaseq-app:v0.4.0
    repo: https://github.com/nesi/training-environment-rstudio-scrnaseq-app.git
    version: 'v0.4.0'
    enabled: true
    pre_pull: true

  rstudio:
    k8s_container: ghcr.io/nesi/training-environment-rstudio-app:v0.3.0
    repo: https://github.com/nesi/training-environment-rstudio-app.git
    version: 'v0.3.0'
    enabled: true
    pre_pull: false

  shell4b:
    k8s_container: ghcr.io/nesi/training-environment-jupyter-intermediate-shell-app:v0.3.3
    repo: https://github.com/nesi/training-environment-jupyter-intermediate-shell-app.git
    version: 'v0.3.3'
    enabled: true
    pre_pull: false

  containers:
    k8s_container: ghcr.io/nesi/training-environment-jupyter-containers-app:v0.1.0
    repo: https://github.com/nesi/training-environment-jupyter-containers-app.git
    version: 'v0.1.0'
    enabled: false
    pre_pull: false

# this is currently required for containers app to run properly (fakeroot)
# Note: you should probably set to false unless you are running a containers workshop
enable_privileged_pods: false

# pull the images defined in ood_apps onto all k8s worker nodes
# Note: make sure the worker nodes have enough disk space (especially if many apps are enabled)
enable_pod_pre_pull: true

# variables to provision data into home dirs (for apps where the data is large)
provision_data_scrnaseq: true

# tuning the web node apache config, setting to 8 gives the suggested setting here:
# https://osc.github.io/ood-documentation/latest/how-tos/debug/debug-apache.html#performance-tuning
# should be good for 40-50 users
# in production you should choose a web node flavor that has at least as many CPUs
# as the value you set here
apache_server_limit: 14

# the number of nfs threads to start, default to 64
# if your app does heavy IO it *may* help to increase this and possibly also increase size
# of services node flavour
nfs_threads: 128

###############################################################################
# variables below this line should not usually need to be changed
###############################################################################

slack_notify: yes
slack_token: "CHANGEME_SLACK_TOKEN"
keycloak_admin_password: "CHANGEME_KEYCLOAK_ADMIN_PASSWORD"
ldap_admin_password: "CHANGEME_LDAP_ADMIN_PASSWORD"

# DNS Settings
route_53_zone: data.nesi.org.nz

oidc_hostname: "{{ hostvars['servicesnode']['hostname'] }}.{{ route_53_zone }}"
clouds_yaml_local_location: ~/.config/openstack/clouds.yaml

cluster_rdc_project: NeSI-Training-Test

# authentication settings
oidc_uri: "/oidc"
oidc_settings_samefile: true
oidc_provider_metadata_url: https://{{ oidc_hostname }}/realms/ondemand/.well-known/openid-configuration
oidc_client_id: ondemand.flexi
oidc_remote_user_claim: preferred_username
oidc_scope: "openid profile email"
oidc_session_inactivity_timeout: 28800
oidc_session_max_duration: 28800
oidc_cookie_same_site: "On"
oidc_settings:
  OIDCCryptoPassphrase: "CHANGEME_OIDC_CRYPTO_PASSPHRASE"
  OIDCSSLValidateServer: 'Off'
  OIDCOAuthSSLValidateServer: 'Off'
  OIDCPassClaimsAs: environment
  OIDCPassRefreshToken: 'On'
  OIDCStripCookies: mod_auth_openidc_session mod_auth_openidc_session_chunks mod_auth_openidc_session_0 mod_auth_openidc_session_1

# ood portal config
servername: "{{ hostvars['webnode']['hostname'] }}.{{ route_53_zone }}"
ssl_cert: /etc/letsencrypt/certs/{{ servername }}.crt
ssl_cert_key: /etc/letsencrypt/keys/{{ servername }}.key
ssl:
- "SSLCertificateFile {{ ssl_cert }}"
- "SSLCertificateKeyFile {{ ssl_cert_key }}"
httpd_auth:
  - 'AuthType openid-connect'
  - 'Require valid-user'
node_uri: '/node'
rnode_uri: '/rnode'
logout_redirect: '/oidc?logout=https%3A%2F%2F{{ servername }}'
pun_pre_hook_exports: 'OIDC_ACCESS_TOKEN,OIDC_CLAIM_EMAIL,OIDC_REFRESH_TOKEN'

cancel_session_enabled: true

httpd_port: 443
server_aliases:
  - webnode.flexi.nesi

# clusters config
clusters:
  local_linux_cluster: |
    ---
    v2:
      metadata:
        title: "Webnode"
        hidden: false
      login:
        host: 'webnode.flexi.nesi'
      job:
        adapter: "linux_host"
        submit_host: "webnode.flexi.nesi"
        ssh_hosts:
          - webnode.flexi.nesi
          - webnode
        site_timeout: 72000
        debug: true
        singularity_bin: /usr/bin/singularity
        singularity_bindpath: /etc,/media,/mnt,/opt,/run,/srv,/usr,/var
        singularity_image: /opt/ood/linuxhost_adapter/ubuntu_22.04.sif
        strict_host_checking: false
        tmux_bin: /usr/bin/tmux

# HTTPS LetsEncrypt Settings

acme_directory: https://acme-v02.api.letsencrypt.org/directory
#acme_directory: https://acme-staging-v02.api.letsencrypt.org/directory

# Clusterctl config

k8s_ood_enable: true
enable_gpu_nodes: false
use_job_pod_reaper: false

kubernetes_version: v1.28.5
capi_image_name: rocky-93-kube-v1.28.5

capi_provider_version: v0.8.0

cluster_name: "{{ terraform_workspace }}"
cluster_namespace: default

openstack_ssh_key: CHANGEME_OPENSTACK_SSH_KEY_NAME

cluster_control_plane_count: 1

cluster_node_cidr: 10.1.0.0/24
cluster_pod_cidr: 172.0.0.0/16

bin_dir: /usr/local/bin

clusterctl_version: v1.5.1
clusterctl_repo: 'https://github.com/kubernetes-sigs/cluster-api/releases/download/{{ clusterctl_version }}/clusterctl-linux-amd64'

## Variables for OpenID Connect Configuration https://kubernetes.io/docs/admin/authentication/
## To use OpenID you have to deploy additional an OpenID Provider (e.g Dex, Keycloak, ...)

kube_oidc_auth: true
kube_oidc_url: https://{{ oidc_hostname }}/realms/ondemand
kube_oidc_client_id: kubernetes
## Optional settings for OIDC
kube_oidc_username_claim: "{{ oidc_remote_user_claim }}"
kube_oidc_username_prefix: "-"
kube_oidc_groups_claim: groups
kube_oidc_groups_prefix: 'oidc:'

# Stuff to not change

ca_file_path: /usr/local/share/ca-certificates

rdc_private_key_path: "{{ ca_file_path }}/rdc-CA.key"
rdc_certificate_path: "{{ ca_file_path }}/rdc-CA.crt"
